---
title: "Getting Started with Reinforcement Learning for Control Systems"
date: "2026-01-10"
excerpt: "An introduction to applying reinforcement learning algorithms for real-time control in particle physics experiments."
tags: ["machine-learning", "reinforcement-learning", "python", "research"]
published: true
---

Reinforcement learning (RL) has emerged as a powerful paradigm for solving complex control problems. In this post, I'll share some insights from my work applying RL to control systems at Fermilab.

## The Challenge

Traditional control systems in particle physics rely on PID controllers and model predictive control (MPC). While effective, these approaches struggle with:

- Nonlinear dynamics that are difficult to model
- Time-varying system parameters
- High-dimensional state and action spaces

## Mathematical Framework

The core of RL is the Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \gamma)$:

- $S$: State space
- $A$: Action space
- $P(s'|s,a)$: Transition probability
- $R(s,a)$: Reward function
- $\gamma$: Discount factor

The goal is to find a policy $\pi(a|s)$ that maximizes the expected cumulative reward:

$$
J(\pi) = \mathbb{E}_{\tau \sim \pi}\left[\sum_{t=0}^{\infty} \gamma^t R(s_t, a_t)\right]
$$

## Implementation

Here's a simple example of a policy network in PyTorch:

```python title="policy_network.py"
import torch
import torch.nn as nn
import torch.nn.functional as F

class PolicyNetwork(nn.Module):
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

    def forward(self, state: torch.Tensor) -> torch.Tensor:
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        return torch.tanh(self.fc3(x))  # Actions in [-1, 1]
```

The training loop follows the standard actor-critic pattern:

```python
def train_step(policy, critic, optimizer, batch):
    states, actions, rewards, next_states = batch

    # Compute TD target
    with torch.no_grad():
        next_values = critic(next_states)
        targets = rewards + gamma * next_values

    # Update critic
    values = critic(states)
    critic_loss = F.mse_loss(values, targets)

    # Update policy
    log_probs = policy.log_prob(states, actions)
    advantages = targets - values.detach()
    policy_loss = -(log_probs * advantages).mean()

    optimizer.zero_grad()
    (critic_loss + policy_loss).backward()
    optimizer.step()
```

## Key Insights

<Callout type="info" title="Reward Shaping Matters">
  The choice of reward function significantly impacts learning stability. Dense rewards with proper scaling lead to faster convergence than sparse terminal rewards.
</Callout>

From our experiments, we found that:

1. **State normalization** is crucial for stable training
2. **Action smoothing** helps with physical actuator constraints
3. **Domain randomization** improves transfer to real hardware

## Results

Our RL controller achieved a **23% reduction** in beam position error compared to the baseline PID controller, while maintaining stability under varying operating conditions.

| Metric | PID Controller | RL Controller | Improvement |
|--------|---------------|---------------|-------------|
| Mean Error (μm) | 12.4 | 9.5 | 23% |
| Max Error (μm) | 45.2 | 31.8 | 30% |
| Response Time (ms) | 150 | 95 | 37% |

## What's Next

I'm currently exploring:

- **Model-based RL** to improve sample efficiency
- **Multi-agent coordination** for distributed control
- **Safe RL** with constraint satisfaction guarantees

Stay tuned for more updates on this research.
